{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c054d9f45553fe6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.543657Z",
     "start_time": "2025-05-20T10:43:54.538671Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Add the /KITE/unet/src folder to Python path so you can import pytorch_unet\n",
    "src_path = '/Users/yamacomur/Desktop/KITE/unet/src'\n",
    "if src_path not in sys.path:\n",
    "    sys.path.append(src_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.563395Z",
     "start_time": "2025-05-20T10:43:54.559362Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.modules.loss import CrossEntropyLoss\n",
    "from scipy.ndimage import zoom\n",
    "from collections import defaultdict\n",
    "import cv2 as cv\n",
    "from pytorch_unet import UNet, DiceLoss_TUnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c1a081123787e9d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.587228Z",
     "start_time": "2025-05-20T10:43:54.578977Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed is set\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def seed_everything(seed=1234):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything()\n",
    "\n",
    "print(\"Seed is set\")\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46397581c60e8e3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.615827Z",
     "start_time": "2025-05-20T10:43:54.598524Z"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'config' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpytorch_unet\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m UNet, DiceLoss_TUnet \n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Initialize model\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m model \u001b[38;5;241m=\u001b[39m UNet(n_class\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_class\u001b[39m\u001b[38;5;124m\"\u001b[39m], f_size\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_map_size\u001b[39m\u001b[38;5;124m\"\u001b[39m], task_no\u001b[38;5;241m=\u001b[39mconfig[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_task\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# Loss functions\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'config' is not defined"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_unet import UNet, DiceLoss_TUnet \n",
    "\n",
    "# Initialize model\n",
    "model = UNet(n_class=config[\"num_class\"], f_size=config[\"feature_map_size\"], task_no=config[\"num_task\"])\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss functions\n",
    "dice_loss = DiceLoss_TUnet(n_classes=config[\"num_class\"])\n",
    "bce_loss = nn.CrossEntropyLoss(reduction=\"none\")  # for pixel-wise loss\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = Adam(model.parameters(), lr=config[\"lr\"], weight_decay=1e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=10)\n",
    "\n",
    "print(\"UNet model, loss, and optimizer ready\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61d760f4eb7153f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.635972Z",
     "start_time": "2025-05-20T10:43:54.627365Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 as cv\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import numpy as np\n",
    "from scipy.ndimage import zoom\n",
    "from torchvision import transforms\n",
    "\n",
    "class OCTDataset(Dataset):\n",
    "    def __init__(self, image_dir, label_dir, file_list_path, input_size=(224, 512), transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.label_dir = label_dir\n",
    "        self.input_size = input_size\n",
    "        self.transform = transform or transforms.ToTensor()\n",
    "\n",
    "        with open(file_list_path, 'r') as f:\n",
    "            self.filenames = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        filename = self.filenames[idx]\n",
    "\n",
    "        img_path = os.path.join(self.image_dir, filename)\n",
    "        label_path = os.path.join(self.label_dir, filename)\n",
    "\n",
    "        image = cv.imread(img_path, 0)  # grayscale\n",
    "        # Load label\n",
    "        label = cv.imread(label_path, 0)  # Grayscale\n",
    "        \n",
    "        # No need to binarize - keep the original class values (0-9)\n",
    "        # Just make sure values are in the correct range\n",
    "        assert label.min() >= 0 and label.max() < num_classes, f\"Label values out of range: min={label.min()}, max={label.max()}\"\n",
    "\n",
    "        if image is None:\n",
    "            raise FileNotFoundError(f\"Image not found or unreadable: {img_path}\")\n",
    "        if label is None:\n",
    "            raise FileNotFoundError(f\"Label not found or unreadable: {label_path}\")\n",
    "\n",
    "        # Resize if needed\n",
    "        if image.shape != self.input_size:\n",
    "            image = zoom(image, (self.input_size[0]/image.shape[0], self.input_size[1]/image.shape[1]), order=3)\n",
    "        if label.shape != self.input_size:\n",
    "            label = zoom(label, (self.input_size[0]/label.shape[0], self.input_size[1]/label.shape[1]), order=0)\n",
    "\n",
    "        # CHANGE: Ensure label is binary (0/1)\n",
    "        label = (label > 0).astype(np.uint8)\n",
    "        \n",
    "        # CHANGE: Better normalization for the image\n",
    "        if np.std(image) > 0:\n",
    "            image = (image - np.min(image)) / (np.max(image) - np.min(image))\n",
    "        else:\n",
    "            image = image / 255.0\n",
    "\n",
    "        # Convert to tensor\n",
    "        image_tensor = torch.from_numpy(image).unsqueeze(0).float()  # shape: [1, H, W]\n",
    "        label_tensor = torch.tensor(label, dtype=torch.long) \n",
    "\n",
    "        return image_tensor, label_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "58814d42d61e05cf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.650932Z",
     "start_time": "2025-05-20T10:43:54.646726Z"
    }
   },
   "outputs": [],
   "source": [
    "#paths\n",
    "image_dir = \"/Users/yamacomur/Desktop/Spring 2025/COMP491/Data/duke_original/image\"\n",
    "label_dir = \"/Users/yamacomur/Desktop/Spring 2025/COMP491/Data/duke_original/layer\"\n",
    "file_list_path = \"/Users/yamacomur/Desktop/KITE/unet/contains_lesion/fold1/train.txt\"\n",
    "val_file_list_path = \"/Users/yamacomur/Desktop/KITE/unet/contains_lesion/fold1/val.txt\"\n",
    "\n",
    "# datasets\n",
    "train_dataset = OCTDataset(image_dir=image_dir, label_dir=label_dir, file_list_path=file_list_path)\n",
    "val_dataset = OCTDataset(image_dir=image_dir, label_dir=label_dir, file_list_path=val_file_list_path)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=4, shuffle=True, worker_init_fn=seed_worker)\n",
    "val_loader = DataLoader(val_dataset, batch_size=4, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "dataloaders = {\n",
    "    \"train\": train_loader,\n",
    "    \"val\": val_loader\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a44fba04152b1a73",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.666311Z",
     "start_time": "2025-05-20T10:43:54.660190Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import copy\n",
    "\n",
    "def calc_loss(pred, target, loss_fn, metrics, phase):\n",
    "    bce = loss_fn(pred, target)\n",
    "    loss = bce\n",
    "\n",
    "    metrics['bce'] += bce.item() * target.size(0)\n",
    "    metrics['loss'] += loss.item() * target.size(0)\n",
    "\n",
    "    return loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    print(f\"--- {phase.upper()} ---\")\n",
    "    for k in metrics.keys():\n",
    "        print(f\"{k}: {metrics[k] / epoch_samples:.4f}\")\n",
    "    print(\"---------------\")\n",
    "\n",
    "def train_model(model, dataloaders, optimizer, loss_fn, scheduler, num_epochs=10):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        print(f\"\\nEpoch {epoch}/{num_epochs}\")\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device).float()\n",
    "                labels = labels.to(device).long()\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    loss = calc_loss(outputs, labels, loss_fn, metrics, phase)\n",
    "\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            if phase == 'val':\n",
    "                scheduler.step(epoch_loss)\n",
    "                if epoch_loss < best_loss:\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                    print(\"Best model updated\")\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "\n",
    "    print(f\"\\nTraining complete. Best val loss: {best_loss:.4f}\")\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ffe3c3c9fbae7a09",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.692119Z",
     "start_time": "2025-05-20T10:43:54.674979Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UNet model, loss, and optimizer ready \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from pytorch_unet import UNet, DiceLoss_TUnet\n",
    "\n",
    "# Configuration\n",
    "num_classes = 10          # foreground + background\n",
    "num_tasks = 1                  # we're only doing segmentation\n",
    "feature_map_size = 16\n",
    "lr = 0.001\n",
    "\n",
    "# Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Model\n",
    "model = UNet(n_class=num_classes, f_size=feature_map_size, task_no=num_tasks)\n",
    "model = model.to(device)\n",
    "\n",
    "# Loss function\n",
    "loss_fn = DiceLoss_TUnet(n_classes=num_classes)\n",
    "\n",
    "# Optimizer & scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "\n",
    "# Better scheduler with faster response\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=5, verbose=True, min_lr=1e-6\n",
    ")\n",
    "\n",
    "print(\"UNet model, loss, and optimizer ready \")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991bcab6f018f844",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T10:43:54.708135Z",
     "start_time": "2025-05-20T10:43:54.702210Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "\n",
    "def multiclass_loss(outputs, targets):\n",
    "    # Cross-entropy loss\n",
    "    ce_loss = F.cross_entropy(outputs, targets)\n",
    "    \n",
    "    # Dice loss for multi-class\n",
    "    dice_loss_val = DiceLoss_TUnet(n_classes=num_classes)(outputs, targets)\n",
    "    \n",
    "    # If dice_loss returns per-class values, reduce to scalar\n",
    "    if isinstance(dice_loss_val, torch.Tensor) and dice_loss_val.numel() > 1:\n",
    "        dice_loss_val = dice_loss_val.mean()\n",
    "    \n",
    "    # Combined loss (adjust weights as needed)\n",
    "    loss = 0.5 * ce_loss + 0.5 * dice_loss_val\n",
    "    return loss\n",
    "\n",
    "# Set as your loss function\n",
    "loss_fn = multiclass_loss\n",
    "\n",
    "def train_model(model, dataloaders, optimizer, loss_fn, scheduler, num_epochs=100):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = float('inf')\n",
    "    history = {'train_loss': [], 'val_loss': []}\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "\n",
    "            running_loss = 0.0\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    # Forward pass\n",
    "                    outputs = model(inputs)\n",
    "                    \n",
    "                    # Get loss - but ensure it's a scalar\n",
    "                    # If your loss_fn returns a tensor, reduce it to a scalar\n",
    "                    raw_loss = loss_fn(outputs, labels)\n",
    "                    \n",
    "                    # IMPORTANT FIX: Make sure the loss is a scalar\n",
    "                    if isinstance(raw_loss, torch.Tensor) and raw_loss.numel() > 1:\n",
    "                        loss = raw_loss.mean()  # Use mean to reduce to scalar\n",
    "                    else:\n",
    "                        loss = raw_loss\n",
    "                    \n",
    "                    # Backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()  # This will now work because loss is a scalar\n",
    "                        optimizer.step()\n",
    "\n",
    "                # Statistics\n",
    "                batch_size = inputs.size(0)\n",
    "                running_loss += loss.item() * batch_size\n",
    "                epoch_samples += batch_size\n",
    "\n",
    "            # Print epoch statistics\n",
    "            epoch_loss = running_loss / epoch_samples\n",
    "            print(f\"{phase.capitalize()} Loss: {epoch_loss:.4f}\")\n",
    "            \n",
    "            # Store history\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "            else:\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                # Update scheduler\n",
    "                scheduler.step(epoch_loss)\n",
    "                # Check if this is the best model\n",
    "                if epoch_loss < best_loss:\n",
    "                    print(\"Saving best model...\")\n",
    "                    best_loss = epoch_loss\n",
    "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "    # Load best weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    print(\"\\nTraining complete. Best val loss: {:.4f}\".format(best_loss))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8ef104c37a72a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:44:35.684203Z",
     "start_time": "2025-05-20T11:04:45.621347Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n",
      "------------------------------\n",
      "Train Loss: 0.8513\n",
      "Val Loss: 0.3515\n",
      "Saving best model...\n",
      "\n",
      "Epoch 2/10\n",
      "------------------------------\n",
      "Train Loss: 0.3250\n",
      "Val Loss: 0.3515\n",
      "Saving best model...\n",
      "\n",
      "Epoch 3/10\n",
      "------------------------------\n",
      "Train Loss: 0.3228\n",
      "Val Loss: 0.3515\n",
      "Saving best model...\n",
      "\n",
      "Epoch 4/10\n",
      "------------------------------\n",
      "Train Loss: 0.3249\n",
      "Val Loss: 0.3515\n",
      "Saving best model...\n",
      "\n",
      "Epoch 5/10\n",
      "------------------------------\n",
      "Train Loss: 0.3249\n",
      "Val Loss: 0.3515\n",
      "\n",
      "Epoch 6/10\n",
      "------------------------------\n",
      "Train Loss: 0.3335\n",
      "Val Loss: 0.3515\n",
      "\n",
      "Epoch 7/10\n",
      "------------------------------\n",
      "Train Loss: 0.3356\n",
      "Val Loss: 0.3515\n",
      "Epoch 00007: reducing learning rate of group 0 to 5.0000e-04.\n",
      "\n",
      "Epoch 8/10\n",
      "------------------------------\n",
      "Train Loss: 0.3462\n",
      "Val Loss: 0.3515\n",
      "\n",
      "Epoch 9/10\n",
      "------------------------------\n",
      "Train Loss: 0.3334\n",
      "Val Loss: 0.3515\n",
      "\n",
      "Epoch 10/10\n",
      "------------------------------\n",
      "Train Loss: 0.3292\n",
      "Val Loss: 0.3515\n",
      "\n",
      "Training complete. Best val loss: 0.3515\n",
      " Trained UNet model saved to: /Users/yamacomur/Desktop/KITE/unet/notebooks/unet_traced.pt\n"
     ]
    }
   ],
   "source": [
    "# Run training\n",
    "trained_model = train_model(\n",
    "    model=model,\n",
    "    dataloaders=dataloaders,\n",
    "    optimizer=optimizer,\n",
    "    loss_fn=loss_fn,\n",
    "    scheduler=scheduler,\n",
    "    num_epochs=10 #CHANGE IT\n",
    ")\n",
    "\n",
    "# Save the trained model in TorchScript format\n",
    "traced_model = torch.jit.trace(trained_model.cpu(), torch.rand(1, 1, 224, 512))\n",
    "save_path = \"/Users/yamacomur/Desktop/KITE/unet/notebooks/unet_traced.pt\"\n",
    "traced_model.save(save_path)\n",
    "\n",
    "print(f\" Trained UNet model saved to: {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58e044c7e6acc867",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-05-20T11:45:07.624471Z",
     "start_time": "2025-05-20T11:45:06.968481Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traced UNet model saved to: /Users/yamacomur/Desktop/KITE/unet/notebooks/unet_traced.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "model = model.cpu()\n",
    "model.eval()\n",
    "\n",
    "dummy_input = torch.rand(1, 1, 224, 512)\n",
    "\n",
    "traced_model = torch.jit.trace(model, dummy_input)\n",
    "\n",
    "save_path = \"/Users/yamacomur/Desktop/KITE/unet/notebooks/unet_traced.pt\"\n",
    "traced_model.save(save_path)\n",
    "\n",
    "print(f\"Traced UNet model saved to: {save_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
